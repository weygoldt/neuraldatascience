{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Neural Data Science_\n",
    "\n",
    "Lecturer: Prof. Dr. Philipp Berens\n",
    "\n",
    "Tutors: Jonas Beck, Ziwei Huang, Rita González Márquez\n",
    "\n",
    "Summer term 2023\n",
    "\n",
    "Student names: *FILL IN YOUR NAMES HERE*\n",
    "\n",
    "# Coding Lab 2\n",
    "\n",
    "- __Data__: Use the saved data `nds_cl_1_*.npy` from Coding Lab 1. Or, if needed, download the data files ```nds_cl_1_*.npy``` from ILIAS and save it in the subfolder ```../data/```.\n",
    "- __Dependencies__: You don't have to use the exact versions of all the dependencies in this notebook, as long as they are new enough. But if you run \"Run All\" in Jupyter and the boilerplate code breaks, you probably need to upgrade them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from IPython import embed\n",
    "# from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from plotstyle import PlotStyle\n",
    "\n",
    "    ps = PlotStyle()\n",
    "except:\n",
    "    plt.style.use(\"../matplotlib_style.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace by path to your solutions\n",
    "import pathlib\n",
    "\n",
    "datapath = pathlib.Path(\"../data\")\n",
    "pc1s = np.load(datapath / \"nds_cl_1_features.npy\")\n",
    "spiketimes = np.load(datapath / \"nds_cl_1_spiketimes_s.npy\")\n",
    "waveforms = np.load(datapath / \"nds_cl_1_waveforms.npy\")\n",
    "np.random.seed(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Generate toy data\n",
    "\n",
    "Sample 1000 data points from a two dimensional mixture of Gaussian model with three clusters  and the following parameters:\n",
    "\n",
    "$\\mu_1 = \\begin{bmatrix}0\\\\0\\end{bmatrix}, \\Sigma_1 = \\begin{bmatrix}1 & 0\\\\0 & 1\\end{bmatrix}, \\pi_1=0.3$\n",
    "\n",
    "$\\mu_2 = \\begin{bmatrix}5\\\\1\\end{bmatrix}, \\Sigma_2 = \\begin{bmatrix}2 & 1\\\\1 & 2\\end{bmatrix}, \\pi_2=0.5$\n",
    "\n",
    "$\\mu_3 = \\begin{bmatrix}0\\\\4\\end{bmatrix}, \\Sigma_3 = \\begin{bmatrix}1 & -0.5\\\\-0.5 & 1\\end{bmatrix}, \\pi_3=0.2$\n",
    "\n",
    "Plot the sampled data points and indicate in color the cluster each point came from. Plot the cluster means as well.\n",
    "\n",
    "*Grading: 1 pts*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(\n",
    "    N: int, m: np.ndarray, S: np.ndarray, p: np.ndarray\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generate N samples from a Mixture of Gaussian distribution with\n",
    "    means m, covariances S and priors p.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    N: int\n",
    "        Number of samples\n",
    "\n",
    "    m: np.ndarray, (n_clusters, n_dims)\n",
    "        Means\n",
    "\n",
    "    S: np.ndarray, (n_clusters, n_dims, n_dims)\n",
    "        Covariances\n",
    "\n",
    "    p: np.ndarray, (n_clusters, )\n",
    "        Cluster weights / probablities\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    labels: np.array, (n_samples, )\n",
    "        Grund truth labels.\n",
    "\n",
    "    x: np.array, (n_samples, n_dims)\n",
    "        Data points\n",
    "    \"\"\"\n",
    "\n",
    "    # make k cluster labels\n",
    "    labels = np.arange(0, len(m[:, 0]))\n",
    "\n",
    "    # itialize empty array to store the data\n",
    "    data_values = np.zeros((N, len(m[0])))\n",
    "    data_labels = np.zeros(N)\n",
    "\n",
    "    # loop over the number of samples\n",
    "    for iter in range(N):\n",
    "        # assing the current sample to one of the clusters with probability p\n",
    "        cluster = np.random.choice(labels, p=p)\n",
    "        data_labels[iter] = cluster\n",
    "\n",
    "        # sample from a 2D gaussian with mean m[k] and covariance S[k]\n",
    "        data_values[iter, :] = np.random.multivariate_normal(m[cluster], S[cluster])\n",
    "\n",
    "    return data_labels, data_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5000  # total number of samples\n",
    "\n",
    "cluster_probs = np.array([0.3, 0.5, 0.2])  # percentage of each cluster\n",
    "cluster_means = np.array([[0.0, 0.0], [5.0, 1.0], [0.0, 4.0]])  # means\n",
    "\n",
    "S1 = np.array([[1.0, 0.0], [0.0, 1.0]])\n",
    "S2 = np.array([[2.0, 1.0], [1.0, 2.0]])\n",
    "S3 = np.array([[1.0, -0.5], [-0.5, 1.0]])\n",
    "cluster_covs = np.stack([S1, S2, S3])  # cov\n",
    "\n",
    "data_labels, data_values = sample_data(\n",
    "    num_samples, cluster_means, cluster_covs, cluster_probs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\", \"tab:brown\"]\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, label in enumerate(np.unique(data_labels)):\n",
    "    ax.scatter(\n",
    "        data_values[data_labels == label, 0],\n",
    "        data_values[data_labels == label, 1],\n",
    "        marker=\".\",\n",
    "        linewidths=0,\n",
    "        s=20,\n",
    "        c=colors[i],\n",
    "    )\n",
    "ax.scatter(cluster_means[:, 0], cluster_means[:, 1], marker=\"o\", s=50, c=\"red\")\n",
    "ax.axvline(x=0, c=\"gray\", linewidth=1, alpha=0.5, zorder=-1)\n",
    "ax.axhline(y=0, c=\"gray\", linewidth=1, alpha=0.5, zorder=-1)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"Data points from a mixture of 3 Gaussians\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement a Gaussian mixture model\n",
    "\n",
    "Implement the EM algorithm to fit a Gaussian mixture model in `fit_mog()`.  Sort the data points by inferring their class labels from your mixture model (by using maximum a-posteriori classification). Fix the seed of the random number generator to ensure deterministic and reproducible behavior. Test it on the toy dataset specifying the correct number of clusters and make sure the code works correctly. Plot the data points from the toy dataset and indicate in color the cluster each point was assigned to by your model. How does the assignment compare to ground truth? If you run the algorithm multiple times, you will notice that some solutions provide suboptimal clustering solutions - depending on your initialization strategy.  \n",
    "\n",
    "*Grading: 4 pts*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM2D:\n",
    "    def __init__(self, k, niters=10):\n",
    "\n",
    "        self.k = k\n",
    "        self.niters = niters\n",
    "    \n",
    "    def initialize(self, x):\n",
    "        self.shape = x.shape\n",
    "        self.n, self.d = self.shape\n",
    "        \n",
    "        self.prior_probs = np.ones(self.k) / self.k\n",
    "        self.posterior_probs = np.ones(self.k) / self.k\n",
    "        kmeans = KMeans(n_clusters=self.k, n_init='auto').fit(x)\n",
    "        self.means = kmeans.cluster_centers_\n",
    "        # self.means = np.random.choice(x.flatten(), (self.k, self.d))\n",
    "        # self.cov = np.asarray([np.eye(self.d) for _ in range(self.k)])\n",
    "        self.cov = np.asarray([np.cov(x.T) for _ in range(self.k)])\n",
    "\n",
    "    def fit(self, x):\n",
    "        self.initialize(x)\n",
    "        for iter in range(self.niters):\n",
    "            self.__expectation_step(x)\n",
    "            self.__maximization_step(x)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.argmax(self.__predict_probability(x), axis=1)\n",
    "\n",
    "    def __predict_probability(self, x):\n",
    "\n",
    "        # compute the likelihood of each data point that it comes from each of\n",
    "        # n clusters\n",
    "        likelihoods = np.zeros((self.n, self.k))\n",
    "        for cluster in range(self.k):\n",
    "            distribution = sp.stats.multivariate_normal(\n",
    "                mean=self.means[cluster], \n",
    "                cov=self.cov[cluster]\n",
    "            )\n",
    "            likelihoods[:, cluster] = distribution.pdf(x)\n",
    "        \n",
    "        # update the posterior probability of each cluster (probabilities of\n",
    "        # the data to have been drawn from each of the gaussians) \n",
    "        num = likelihoods * self.prior_probs\n",
    "        denom = np.sum(num, axis=1)[:, np.newaxis]\n",
    "        posterior_probs = num / denom\n",
    "\n",
    "        return posterior_probs\n",
    "\n",
    "    def __expectation_step(self, x):\n",
    "        self.posterior_probs = self.__predict_probability(x)\n",
    "        self.prior_probs = np.mean(self.posterior_probs, axis=0)\n",
    "    \n",
    "    def __maximization_step(self, x):\n",
    "        epsilon = 1e-5 # a small number to avoid division by zero\n",
    "        for cluster in range(self.k):  \n",
    "            # compute the posterior probability of each cluster (probabilities of \n",
    "            # the data to have been drawn from each of the gaussians) \n",
    "            # by updating our initial guess (the prior probabilities) with \n",
    "            # the likelihoods\n",
    "            posterior_prob = self.posterior_probs[:, cluster]\n",
    "            total_posterior_prob = np.sum(posterior_prob)\n",
    "            self.means[cluster] = np.sum(posterior_prob * x.T, axis=1) / (total_posterior_prob + epsilon)\n",
    "            self.cov[cluster] = np.cov(\n",
    "                x.T, \n",
    "                aweights=(posterior_prob / (total_posterior_prob + epsilon)),\n",
    "                bias=True\n",
    "            ) + np.identity(self.d) * epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Mixture of Gaussian on toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "gmm = GMM2D(3, niters=30)\n",
    "gmm.fit(data_values)\n",
    "pred_labels = gmm.predict(data_values)\n",
    "\n",
    "print(f\"Predicted means: {gmm.means}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot toy data with cluster assignments and compare to original labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic = [[\"True\", \"MoG\"]]\n",
    "fig, ax = plt.subplot_mosaic(mosaic=mosaic, figsize=(8, 4), layout=\"constrained\")\n",
    "\n",
    "for i, label in enumerate(np.unique(data_labels)):\n",
    "    ax[\"True\"].set_title(\"True labels\")\n",
    "    ax['True'].scatter(\n",
    "        data_values[data_labels == label, 0],\n",
    "        data_values[data_labels == label, 1],\n",
    "        marker=\".\",\n",
    "        linewidths=0,\n",
    "        s=20,\n",
    "        c=colors[i],\n",
    "    )\n",
    "ax['True'].scatter(cluster_means[:, 0], cluster_means[:, 1], marker=\"o\", s=50, c=\"red\")\n",
    "for i, predicted in enumerate(np.unique(pred_labels)):\n",
    "    ax[\"MoG\"].set_title(\"MoG labels\")\n",
    "    ax[\"MoG\"].scatter(\n",
    "        data_values[pred_labels == predicted, 0],\n",
    "        data_values[pred_labels == predicted, 1],\n",
    "        marker=\".\",\n",
    "        linewidths=0,\n",
    "        s=20,\n",
    "        c=colors[i],\n",
    "        alpha=0.5,\n",
    "    )\n",
    "ax[\"MoG\"].scatter(gmm.means[:, 0], gmm.means[:, 1], marker=\"o\", s=50, c=\"red\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Model complexity\n",
    "A priori we do not know how many neurons we recorded. Extend your algorithm with an automatic procedure to select the appropriate number of mixture components (clusters). Base your decision on the Bayesian Information Criterion:\n",
    "\n",
    "$BIC = -2L+P \\log N,$\n",
    "\n",
    "where $L$ is the log-likelihood of the data under the best model, $P$ is the number of parameters of the model and $N$ is the number of data points. You want to minimize the quantity. Plot the BIC as a function of mixture components. What is the optimal number of clusters on the toy dataset?\n",
    "\n",
    "You can also use the BIC to make your algorithm robust against suboptimal solutions due to local minima. Start the algorithm multiple times and pick the best solutions for extra points. You will notice that this depends a lot on which initialization strategy you use.\n",
    "\n",
    "*Grading: 3 pts*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mog_bic(\n",
    "    x: np.ndarray, means: np.ndarray, covs: np.ndarray, probs: np.ndarray\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Compute the BIC for a fitted Mixture of Gaussian model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    x: np.array, (n_samples, n_dims)\n",
    "        Input data\n",
    "\n",
    "    means: np.array, (n_clusters, n_dims)\n",
    "        Means\n",
    "\n",
    "    covs: np.array, (n_clusters, n_dims, n_dims)\n",
    "        Covariances\n",
    "\n",
    "    probs: np.array, (n_clusters, )\n",
    "        Cluster weights / probablities\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "\n",
    "    bic: float\n",
    "        BIC\n",
    "\n",
    "    LL: float\n",
    "        Log Likelihood\n",
    "    \"\"\"\n",
    "\n",
    "    k = means.shape[0]\n",
    "    n = x.shape[0]\n",
    "    n_features = means.shape[1]\n",
    "    p_per_cluster = n_features + n_features**2 + 1 # mean + cov + weight\n",
    "    n_params = k * p_per_cluster\n",
    "\n",
    "    ll = np.sum(np.log(np.sum([p * sp.stats.multivariate_normal.pdf(x, m, c) for p, m, c in zip(probs, means, covs)], axis=0)))\n",
    "    # bic = np.log(x.shape[0]) * (means.shape[0] * (means.shape[1] + 0.5 * means.shape[1] * (means.shape[1] + 1)) + means.shape[0] - 1) - 2 * ll\n",
    "    bic = -2 * ll + n_params * np.log(n) \n",
    "\n",
    "    return bic, ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Compute and plot the BIC for mixture models with different numbers of clusters (e.g., 2 - 6). (0.5 pts)\n",
    "# Make your algorithm robust against local minima. (0.5 pts) and plot the result (0.5 pts)\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "K = range(1, 10)\n",
    "num_seeds = 5\n",
    "seeds = np.random.randint(0, 1000, size=num_seeds)\n",
    "\n",
    "BIC = np.zeros((num_seeds, len(K)))\n",
    "LL = np.zeros((num_seeds, len(K)))\n",
    "\n",
    "for i, k in enumerate(K):\n",
    "    for j, seed in enumerate(seeds):\n",
    "        np.random.seed(seed)\n",
    "        gmm = GMM2D(k, niters=30)\n",
    "        gmm.fit(data_values)\n",
    "        BIC[j, i], LL[j, i] = mog_bic(data_values, gmm.means, gmm.cov, gmm.prior_probs)\n",
    "\n",
    "    print(f\"Computed BIC for {k} clusters: {BIC[:, i].mean():.2f} +/- {BIC[:, i].std():.2f}\")\n",
    "\n",
    "best_k = np.argmin(BIC.mean(axis=0)) + K[0]\n",
    "best_bic = BIC[:, best_k - K[0]].mean()\n",
    "print(f\"--> Best number of clusters: {best_k} with a BIC of {best_bic:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "ax.plot(K, BIC.mean(axis=0), label=\"BIC\")\n",
    "ax.scatter(best_k, best_bic, marker=\"o\", s=50, c=\"red\", zorder = 10)\n",
    "ax.set_ylabel('Bayesian information criterion BIC')\n",
    "ax.set_xlabel('Number of clusters k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Spike sorting using Mixture of Gaussian \n",
    "Run the full algorithm on your set of extracted features (including model complexity selection). Plot the BIC as a function of the number of mixture components on the real data. For the best model, make scatter plots of the first PCs on all four channels (6 plots). Color-code each data point according to its class label in the model with the optimal number of clusters. In addition, indicate the position (mean) of the clusters in your plot. \n",
    "\n",
    "*Grading: 3 pts*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(pc1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# Select the model that best represents the data according to the BIC (include plot) (1 pt)\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "np.random.seed(42)\n",
    "K = np.arange(3, 20)\n",
    "num_seeds = 5\n",
    "seeds = np.random.randint(0, 1000, size=num_seeds)\n",
    "\n",
    "BIC = np.zeros((num_seeds, len(K)))\n",
    "LL = np.zeros((num_seeds, len(K)))\n",
    "\n",
    "for i, k in enumerate(K):\n",
    "    for j, seed in enumerate(seeds):\n",
    "        np.random.seed(seed)\n",
    "        gmm = GMM2D(k, niters=30)\n",
    "        gmm.fit(pc1s)\n",
    "        BIC[j, i], LL[j, i] = mog_bic(pc1s, gmm.means, gmm.cov, gmm.prior_probs)\n",
    "    print(f\"Computed BIC for {k} clusters: {BIC[:, i].mean():.2f} +/- {BIC[:, i].std():.2f}\")\n",
    "\n",
    "best_k = np.argmin(BIC.mean(axis=0)) + K[0]\n",
    "best_bic = BIC[:, best_k - K[0]].mean()\n",
    "best_seed = seeds[np.argmin(BIC[:, best_k - K[0]])]\n",
    "print(f\"--> Best number of clusters: {best_k} with a BIC of {best_bic:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "ax.plot(K, BIC.mean(axis=0), label=\"BIC\")\n",
    "ax.scatter(best_k, best_bic, marker=\"o\", s=50, c=\"red\", zorder = 10)\n",
    "ax.set_ylabel('Bayesian information criterion BIC')\n",
    "ax.set_xlabel('Number of clusters k')\n",
    "\n",
    "# plot BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refit model with lowest BIC and plot data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(best_seed)\n",
    "gmm = GMM2D(best_k, niters=30)\n",
    "gmm.fit(pc1s)\n",
    "labels = gmm.predict(pc1s)\n",
    "means, covs, probs = gmm.means, gmm.cov, gmm.prior_probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mosaic = [\n",
    "    [\"Ch2 vs Ch1\", \".\", \".\"],\n",
    "    [\"Ch3 vs Ch1\", \"Ch3 vs Ch2\", \".\"],\n",
    "    [\"Ch4 vs Ch1\", \"Ch4 vs Ch2\", \"Ch4 vs Ch3\"],\n",
    "]\n",
    "fig, ax = plt.subplot_mosaic(\n",
    "    mosaic=mosaic, figsize=(8, 8), layout=\"constrained\", dpi=100\n",
    ")\n",
    "\n",
    "i = {\"Ch1\": 0, \"Ch2\": 3, \"Ch3\": 6, \"Ch4\": 9}\n",
    "\n",
    "for m in np.ravel(mosaic):\n",
    "    if m == \".\":\n",
    "        continue\n",
    "    # get the indices of the channel for the first channel vs second channel\n",
    "    firstch = i[m[:3]]\n",
    "    secondch = i[m[-3:]]\n",
    "    for k in range(best_k):\n",
    "        # plot the ellipses\n",
    "        # plot_cov_ellipse(S[k, firstch, secondch], m[:3], m[-3:], ax=ax[m], nstd=2, alpha=0.2)\n",
    "        # plot the data points\n",
    "        ax[m].scatter(pc1s[labels == k, firstch], pc1s[labels == k, secondch], s=1, alpha=0.2)\n",
    "\n",
    "    y, x = m.split(\" vs \")\n",
    "\n",
    "    ax[m].set_xlabel(x)\n",
    "    ax[m].set_ylabel(y)\n",
    "    ax[m].set_xlim((-1500, 1500))\n",
    "    ax[m].set_ylim((-1500, 1500))\n",
    "    ax[m].set_xticks([])\n",
    "    ax[m].set_yticks([])\n",
    "\n",
    "fig.suptitle(\"Pairwise 1st PCs\", fontsize=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Cluster separation\n",
    "\n",
    "Implement linear discriminant analysis to visualize how well each cluster is separated from its neighbors in the high-dimensional space in the function `separation()`. Project the spikes of each pair of clusters onto the axis that optimally separates those two clusters. \n",
    "\n",
    "Plot a matrix with pairwise separation plots, showing the histogram of the points in both clusters projected on the axis best separating the clusters (as shown in the lecture). *Hint:* Since Python 3.5+, matrix multiplications can be compactely written as `x@y`.\n",
    "\n",
    "*Grading: 4 pts*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separation(\n",
    "    b: np.ndarray,\n",
    "    m: np.ndarray,\n",
    "    S: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    assignment: np.ndarray,\n",
    "    nbins: int = 50,\n",
    "):\n",
    "    \"\"\"Calculate cluster separation by LDA.\n",
    "\n",
    "    proj, bins = separation(b, m, S, p, assignment)\n",
    "    projects the data on the LDA axis for all pairs of clusters. The result\n",
    "    is normalized such that the left (i.e. first) cluster has\n",
    "    zero mean and unit variances. The LDA axis is estimated from the model.\n",
    "    ---\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    b: np.array, (n_spikes, n_features)\n",
    "        Features.\n",
    "\n",
    "    m: np.array, (n_clusters, n_features)\n",
    "        Means.\n",
    "\n",
    "    S: np.array, (n_clusters, n_features, n_features)\n",
    "        Covariance.\n",
    "\n",
    "    p: np.array, (n_clusters, )\n",
    "        Cluster weight.\n",
    "\n",
    "    assignment: np.array, (n_spikes, )\n",
    "        Cluster assignments / labels for each spike\n",
    "\n",
    "    nbins: int\n",
    "        Number of bins in a lda histogram.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    proj: np.array, (n_bins, n_clusters, n_clusters)\n",
    "        computed lda histogram Comparing the cells in particular\n",
    "\n",
    "    bins: np.array, (n_bins)\n",
    "        bin times relative to center    #bins x 1\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables\n",
    "    n_clusters = len(m)\n",
    "    print(n_clusters)\n",
    "    n_bins = nbins\n",
    "    bin_width = 1 / n_bins\n",
    "    bins = np.linspace(bin_width/2, 1-bin_width/2, n_bins+1)\n",
    "\n",
    "    # Compute pairwise separations\n",
    "    proj = np.zeros((n_bins, n_clusters, n_clusters))\n",
    "    for i in range(n_clusters):\n",
    "        for j in range(i+1, n_clusters):\n",
    "\n",
    "            if j == n_clusters:\n",
    "                j = 0\n",
    "\n",
    "            # Compute the optimal separating axis\n",
    "            Sw = S[i] + S[j]\n",
    "            diff_means = m[j] - m[i]\n",
    "            w = np.linalg.solve(Sw, diff_means)\n",
    "            w_norm = w / np.linalg.norm(w)\n",
    "\n",
    "            # Project the data onto the optimal axis\n",
    "            x = np.dot(b - m[i], w_norm)\n",
    "            x_norm = (x - np.mean(x)) / np.std(x)\n",
    "\n",
    "            # Compute histograms of the projected data\n",
    "            hist, _ = np.histogram(x_norm, bins=bins)\n",
    "            print(i, j)\n",
    "            print(np.shape(proj))\n",
    "            print(np.shape(hist))\n",
    "            proj[:, i, j] = hist / np.sum(hist)\n",
    "    print(proj.shape)\n",
    "    # Symmetrize the histogram matrix\n",
    "    proj += proj.transpose((0, 2, 1))\n",
    "\n",
    "    return proj, bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separation(\n",
    "    features: np.ndarray,\n",
    "    means: np.ndarray,\n",
    "    covs: np.ndarray,\n",
    "    probs: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    nbins: int = 50,\n",
    "):\n",
    "    \"\"\"Calculate cluster separation by LDA.\n",
    "\n",
    "    proj, bins = separation(b, m, S, p, assignment)\n",
    "    projects the data on the LDA axis for all pairs of clusters. The result\n",
    "    is normalized such that the left (i.e. first) cluster has\n",
    "    zero mean and unit variances. The LDA axis is estimated from the model.\n",
    "    ---\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    b: np.array, (n_spikes, n_features)\n",
    "        Features.\n",
    "\n",
    "    m: np.array, (n_clusters, n_features)\n",
    "        Means.\n",
    "\n",
    "    S: np.array, (n_clusters, n_features, n_features)\n",
    "        Covariance.\n",
    "\n",
    "    p: np.array, (n_clusters, )\n",
    "        Cluster weight.\n",
    "\n",
    "    labels: np.array, (n_spikes, )\n",
    "        Cluster assignments / labels for each spike\n",
    "\n",
    "    nbins: int\n",
    "        Number of bins in a lda histogram.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    proj: np.array, (n_bins, n_clusters, n_clusters)\n",
    "        computed lda histogram Comparing the cells in particular\n",
    "    \n",
    "    proj_labels: np.array, (n_clusters, n_clusters)\n",
    "\n",
    "    bins: np.array, (n_bins)\n",
    "        bin times relative to center\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize array to store the histograms\n",
    "    histograms = np.zeros((nbins, len(means), len(means), 2))\n",
    "    overall_mean = np.average(means, weights=probs, axis=0)\n",
    "    # Iterate over all possible pairs of clusters\n",
    "    for i in range(len(means)):\n",
    "        for j in range(len(means)):\n",
    "\n",
    "            # Number of pca features = n_pcs * n_channels\n",
    "            n_features = features.shape[1]\n",
    "\n",
    "            # get the features of the current cluster\n",
    "            bin_features = features[(labels == i) | (labels == j)]\n",
    "            bin_feature_labels = labels[(labels == i) | (labels == j)]\n",
    "\n",
    "            # make label array for the current pair of clusters\n",
    "            # (i.e. 0 for cluster i and 1 for cluster j)\n",
    "            bin_labels = np.asarray([i, j])\n",
    "\n",
    "            # go through each of the two clusters and \n",
    "            # compute the within-cluster scatter matrix\n",
    "            # and the between-cluster scatter matrix\n",
    "\n",
    "            SW = np.zeros((features.shape[1], features.shape[1]))\n",
    "            SB = np.zeros((features.shape[1], features.shape[1]))\n",
    "\n",
    "            # get values to scale sclusters based on \n",
    "            # first cluster\n",
    "            # scale_cov = covs[i]\n",
    "            # scale_mean = means[i]\n",
    "            \n",
    "            # compute the overall mean for the current\n",
    "            # pair of clusters\n",
    "            #overall_mean = (means[i] + means[j]) / 2 #- scale_mean\n",
    "\n",
    "            for bl in bin_labels:\n",
    "\n",
    "                # get the features for the current cluster\n",
    "                m = means[bl] #- scale_mean\n",
    "                cov = covs[bl] #/ scale_cov\n",
    "\n",
    "                # add cov to the within-cluster scatter matrix\n",
    "                SW += cov\n",
    "\n",
    "                # compute overall scatter matrix\n",
    "                mean_diff = m - overall_mean.reshape(n_features, 1)\n",
    "                print(np.shape(mean_diff))\n",
    "                SB += np.dot(mean_diff, mean_diff.T) * len(bin_labels)\n",
    "\n",
    "            # solve the generalized eigenvaue problem\n",
    "            A = np.linalg.inv(SW) @ SB\n",
    "\n",
    "            # Compute eigenvalues and eigenvectors of A\n",
    "            eigvals, eigvecs = np.linalg.eig(A)\n",
    "\n",
    "            # Transpose the eigenvectors\n",
    "            eigvecs = eigvecs.T\n",
    "\n",
    "            # Sort eigenvalues and eigenvectors fron low to high\n",
    "            idx = np.argsort(abs(eigvals))[::-1]\n",
    "            eigvals = eigvals[idx]\n",
    "            eigvecs = eigvecs[idx]\n",
    "\n",
    "            # store the first eigenvector as our LDA axis\n",
    "            lda_axis = eigvecs[0]\n",
    "\n",
    "            # project the data onto the LDA axis\n",
    "            proj = np.dot(bin_features - overall_mean, lda_axis)\n",
    "\n",
    "            # print(np.shape(proj))\n",
    "            # print(np.shape(bin_features))\n",
    "            # print(np.shape(bin_labels))\n",
    "\n",
    "            # # plot the histogram of the projected data for each label\n",
    "            # plt.hist(proj[bin_feature_labels == i], bins=nbins, alpha=0.5)\n",
    "            # plt.hist(proj[bin_feature_labels == j], bins=nbins, alpha=0.5)\n",
    "\n",
    "            # normalize projected data by mean and std of the first cluster\n",
    "            proj = (proj - np.mean(proj[bin_feature_labels == i])) / np.std(proj[bin_feature_labels == i])\n",
    "            \n",
    "            # split the projection into the two clusters\n",
    "            proj_i = proj[bin_feature_labels == i]\n",
    "            proj_j = proj[bin_feature_labels == j]\n",
    "\n",
    "            # make a array for the histogram bins because they \n",
    "            # have to be the same for both clusters\n",
    "            bins = np.linspace(-4, 4, nbins + 1)\n",
    "\n",
    "            # compute the histogram of the projected data\n",
    "            hist_i, _ = np.histogram(proj_i, bins=bins)\n",
    "            hist_j, _ = np.histogram(proj_j, bins=bins)\n",
    "\n",
    "            # put the histograms into the array\n",
    "            histograms[:, i, j, 0] = hist_i \n",
    "            histograms[:, i, j, 1] = hist_j\n",
    "\n",
    "    return histograms, bins\n",
    "\n",
    "\n",
    "\n",
    "         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bins = separation(pc1s, means, covs, probs, labels, nbins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(hist.shape)\n",
    "print(bins.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "# plot the pairwise histograms for each cluster\n",
    "plot_shape = (len(means), len(means))\n",
    "fig, axs = plt.subplots(*plot_shape, figsize=(10, 10), sharex=True, sharey=True)\n",
    "# make array with colors for each cluster\n",
    "colors = [\"blue\", \"red\", \"green\", \"orange\", \"purple\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\", \"magenta\", \"yellow\", \"black\"]\n",
    "# plot the histograms for all possible pairs of clusters\n",
    "for i in range(len(means)):\n",
    "    for j in range(len(means)):\n",
    "\n",
    "        #axs[i, j].bar(bins[:-1], hist[:, i, j, 0], facecolor=\"blue\", alpha=0.5)\n",
    "        #axs[i, j].bar(bins[:-1], hist[:, i, j, 1], facecolor=\"red\", alpha=0.5)\n",
    "        axs[i, j].bar(bins[:-1], hist[:, i, j, 0], facecolor=colors[i], alpha=0.5)\n",
    "        axs[i, j].bar(bins[:-1], hist[:, i, j, 1], facecolor=colors[j], alpha=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
