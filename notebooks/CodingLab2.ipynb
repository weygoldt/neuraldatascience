{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Neural Data Science_\n",
    "\n",
    "Lecturer: Prof. Dr. Philipp Berens\n",
    "\n",
    "Tutors: Jonas Beck, Ziwei Huang, Rita González Márquez\n",
    "\n",
    "Summer term 2023\n",
    "\n",
    "Student names: *FILL IN YOUR NAMES HERE*\n",
    "\n",
    "# Coding Lab 2\n",
    "\n",
    "- __Data__: Use the saved data `nds_cl_1_*.npy` from Coding Lab 1. Or, if needed, download the data files ```nds_cl_1_*.npy``` from ILIAS and save it in the subfolder ```../data/```.\n",
    "- __Dependencies__: You don't have to use the exact versions of all the dependencies in this notebook, as long as they are new enough. But if you run \"Run All\" in Jupyter and the boilerplate code breaks, you probably need to upgrade them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyter_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyter_black\n",
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Last updated: 2023-04-28 09:43:04CEST\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.3\n",
      "IPython version      : 8.11.0\n",
      "\n",
      "sklearn: 0.0.post1\n",
      "\n",
      "matplotlib: 3.7.1\n",
      "scipy     : 1.10.1\n",
      "numpy     : 1.24.3\n",
      "\n",
      "Watermark: 2.3.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from __future__ import annotations\n",
    "\n",
    "%load_ext jupyter_black\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark --time --date --timezone --updated --python --iversions --watermark -p sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"../matplotlib_style.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace by path to your solutions\n",
    "b = np.load(\"../data/nds_cl_1_features.npy\")\n",
    "s = np.load(\"../data/nds_cl_1_spiketimes_s.npy\")\n",
    "w = np.load(\"../data/nds_cl_1_waveforms.npy\")\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Generate toy data\n",
    "\n",
    "Sample 1000 data points from a two dimensional mixture of Gaussian model with three clusters  and the following parameters:\n",
    "\n",
    "$\\mu_1 = \\begin{bmatrix}0\\\\0\\end{bmatrix}, \\Sigma_1 = \\begin{bmatrix}1 & 0\\\\0 & 1\\end{bmatrix}, \\pi_1=0.3$\n",
    "\n",
    "$\\mu_2 = \\begin{bmatrix}5\\\\1\\end{bmatrix}, \\Sigma_2 = \\begin{bmatrix}2 & 1\\\\1 & 2\\end{bmatrix}, \\pi_2=0.5$\n",
    "\n",
    "$\\mu_3 = \\begin{bmatrix}0\\\\4\\end{bmatrix}, \\Sigma_3 = \\begin{bmatrix}1 & -0.5\\\\-0.5 & 1\\end{bmatrix}, \\pi_3=0.2$\n",
    "\n",
    "Plot the sampled data points and indicate in color the cluster each point came from. Plot the cluster means as well.\n",
    "\n",
    "*Grading: 1 pts*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(\n",
    "    N: int, m: np.ndarray, S: np.ndarray, p: np.ndarray\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generate N samples from a Mixture of Gaussian distribution with\n",
    "    means m, covariances S and priors p.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    N: int\n",
    "        Number of samples\n",
    "\n",
    "    m: np.ndarray, (n_clusters, n_dims)\n",
    "        Means\n",
    "\n",
    "    S: np.ndarray, (n_clusters, n_dims, n_dims)\n",
    "        Covariances\n",
    "\n",
    "    p: np.ndarray, (n_clusters, )\n",
    "        Cluster weights / probablities\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    labels: np.array, (n_samples, )\n",
    "        Grund truth labels.\n",
    "\n",
    "    x: np.array, (n_samples, n_dims)\n",
    "        Data points\n",
    "    \"\"\"\n",
    "\n",
    "    # insert your code here\n",
    "    \n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # draw labeled points from mixture of Gaussians (0.5 pt)\n",
    "    # ------------------------------------------------------\n",
    "    # return labels, x\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000  # total number of samples\n",
    "\n",
    "p = np.array([0.3, 0.5, 0.2])  # percentage of each cluster\n",
    "m = np.array([[0.0, 0.0], [5.0, 1.0], [0.0, 4.0]])  # means\n",
    "\n",
    "S1 = np.array([[1.0, 0.0], [0.0, 1.0]])\n",
    "S2 = np.array([[2.0, 1.0], [1.0, 2.0]])\n",
    "S3 = np.array([[1.0, -0.5], [-0.5, 1.0]])\n",
    "S = np.stack([S1, S2, S3])  # cov\n",
    "\n",
    "# labels, x = sample_data(N, m, S, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# plot points from mixture of Gaussians (0.5 pt)\n",
    "# ----------------------------------------------\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement a Gaussian mixture model\n",
    "\n",
    "Implement the EM algorithm to fit a Gaussian mixture model in `fit_mog()`.  Sort the data points by inferring their class labels from your mixture model (by using maximum a-posteriori classification). Fix the seed of the random number generator to ensure deterministic and reproducible behavior. Test it on the toy dataset specifying the correct number of clusters and make sure the code works correctly. Plot the data points from the toy dataset and indicate in color the cluster each point was assigned to by your model. How does the assignment compare to ground truth? If you run the algorithm multiple times, you will notice that some solutions provide suboptimal clustering solutions - depending on your initialization strategy.  \n",
    "\n",
    "*Grading: 4 pts*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_mog(\n",
    "    x: np.ndarray, k: int, niters: int = 10, random_seed: int = 2046\n",
    ") -> tuple[np.ndarray]:\n",
    "    \"\"\"Fit Mixture of Gaussian model using EM algo.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    x: np.array, (n_samples, n_dims)\n",
    "        Input data\n",
    "\n",
    "    k: int\n",
    "        Number of clusters\n",
    "\n",
    "    niters: int\n",
    "        Maximal number of iterations.\n",
    "\n",
    "    random_seed: int\n",
    "        Random Seed\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    labels: np.array, (n_samples)\n",
    "        Cluster labels\n",
    "\n",
    "    m: list or np.array, (n_clusters, n_dims)\n",
    "        Means\n",
    "\n",
    "    S: list or np.array, (n_clusters, n_dims, n_dims)\n",
    "        Covariances\n",
    "\n",
    "    p: list or np.array, (n_clusters, )\n",
    "        Cluster weights / probablities\n",
    "    \"\"\"\n",
    "\n",
    "    # fill in your code here\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # -----------\n",
    "    # init (1 pt)\n",
    "    # -----------\n",
    "\n",
    "    # -------------------------\n",
    "    # EM maximisation (2.5 pts)\n",
    "    # -------------------------\n",
    "\n",
    "    for step in range(niters):\n",
    "        continue\n",
    "        # E step\n",
    "        # Evaluate the posterior probablibities `r`\n",
    "        # using the current values of `m` and `S`\n",
    "\n",
    "        # M step\n",
    "        # Estimate new `m`, `S` and `p`\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Mixture of Gaussian on toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mog_labels, m, S, p = fit_mog(x, 3, random_seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot toy data with cluster assignments and compare to original labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic = [[\"True\", \"MoG\"]]\n",
    "fig, ax = plt.subplot_mosaic(mosaic=mosaic, figsize=(8, 4), layout=\"constrained\")\n",
    "\n",
    "# -----------------\n",
    "# Add plot (0.5 pts)\n",
    "# -----------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Model complexity\n",
    "A priori we do not know how many neurons we recorded. Extend your algorithm with an automatic procedure to select the appropriate number of mixture components (clusters). Base your decision on the Bayesian Information Criterion:\n",
    "\n",
    "$BIC = -2L+P \\log N,$\n",
    "\n",
    "where $L$ is the log-likelihood of the data under the best model, $P$ is the number of parameters of the model and $N$ is the number of data points. You want to minimize the quantity. Plot the BIC as a function of mixture components. What is the optimal number of clusters on the toy dataset?\n",
    "\n",
    "You can also use the BIC to make your algorithm robust against suboptimal solutions due to local minima. Start the algorithm multiple times and pick the best solutions for extra points. You will notice that this depends a lot on which initialization strategy you use.\n",
    "\n",
    "*Grading: 3 pts*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mog_bic(\n",
    "    x: np.ndarray, m: np.ndarray, S: np.ndarray, p: np.ndarray\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Compute the BIC for a fitted Mixture of Gaussian model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    x: np.array, (n_samples, n_dims)\n",
    "        Input data\n",
    "\n",
    "    m: np.array, (n_clusters, n_dims)\n",
    "        Means\n",
    "\n",
    "    S: np.array, (n_clusters, n_dims, n_dims)\n",
    "        Covariances\n",
    "\n",
    "    p: np.array, (n_clusters, )\n",
    "        Cluster weights / probablities\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "\n",
    "    bic: float\n",
    "        BIC\n",
    "\n",
    "    LL: float\n",
    "        Log Likelihood\n",
    "    \"\"\"\n",
    "\n",
    "    # insert your code here\n",
    "\n",
    "    # -------------------------\n",
    "    # implement the BIC (1.5 pts)\n",
    "    # -------------------------\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Compute and plot the BIC for mixture models with different numbers of clusters (e.g., 2 - 6). (0.5 pts)\n",
    "# Make your algorithm robust against local minima. (0.5 pts) and plot the result (0.5 pts)\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "K = range(2, 7)\n",
    "num_seeds = 10\n",
    "\n",
    "BIC = np.zeros((num_seeds, len(K)))\n",
    "LL = np.zeros((num_seeds, len(K)))\n",
    "\n",
    "# run mog and BIC multiple times here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "# plot BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Spike sorting using Mixture of Gaussian \n",
    "Run the full algorithm on your set of extracted features (including model complexity selection). Plot the BIC as a function of the number of mixture components on the real data. For the best model, make scatter plots of the first PCs on all four channels (6 plots). Color-code each data point according to its class label in the model with the optimal number of clusters. In addition, indicate the position (mean) of the clusters in your plot. \n",
    "\n",
    "*Grading: 3 pts*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# Select the model that best represents the data according to the BIC (include plot) (1 pt)\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "K = np.arange(2, 16)\n",
    "num_seeds = 5\n",
    "\n",
    "BIC = np.zeros((num_seeds, len(K)))\n",
    "LL = np.zeros((num_seeds, len(K)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "# plot BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refit model with lowest BIC and plot data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowest BIC: # cluster = 2\n"
     ]
    }
   ],
   "source": [
    "random_seed, kk = np.where(BIC == BIC.min())\n",
    "random_seed = random_seed[0]\n",
    "kk = kk[0]\n",
    "print(f\"lowest BIC: # cluster = {K[kk]}\")\n",
    "# a, m, S, p = fit_mog(b, K[kk], random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "# Create scatterplots of the first PCs under the best model for all 4 channels. (2 pts)\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "mosaic = [\n",
    "    [\"Ch2 vs Ch1\", \".\", \".\"],\n",
    "    [\"Ch3 vs Ch1\", \"Ch3 vs Ch2\", \".\"],\n",
    "    [\"Ch4 vs Ch1\", \"Ch4 vs Ch2\", \"Ch4 vs Ch3\"],\n",
    "]\n",
    "fig, ax = plt.subplot_mosaic(mosaic=mosaic, figsize=(8, 8), layout=\"constrained\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Cluster separation\n",
    "\n",
    "Implement linear discriminant analysis to visualize how well each cluster is separated from its neighbors in the high-dimensional space in the function `separation()`. Project the spikes of each pair of clusters onto the axis that optimally separates those two clusters. \n",
    "\n",
    "Plot a matrix with pairwise separation plots, showing the histogram of the points in both clusters projected on the axis best separating the clusters (as shown in the lecture). *Hint:* Since Python 3.5+, matrix multiplications can be compactely written as `x@y`.\n",
    "\n",
    "*Grading: 4 pts*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separation(\n",
    "    b: np.ndarray,\n",
    "    m: np.ndarray,\n",
    "    S: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    assignment: np.ndarray,\n",
    "    nbins: int = 50,\n",
    "):\n",
    "    \"\"\"Calculate cluster separation by LDA.\n",
    "\n",
    "    proj, bins = separation(b, m, S, p, assignment)\n",
    "    projects the data on the LDA axis for all pairs of clusters. The result\n",
    "    is normalized such that the left (i.e. first) cluster has\n",
    "    zero mean and unit variances. The LDA axis is estimated from the model.\n",
    "    ---\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    b: np.array, (n_spikes, n_features)\n",
    "        Features.\n",
    "\n",
    "    m: np.array, (n_clusters, n_features)\n",
    "        Means.\n",
    "\n",
    "    S: np.array, (n_clusters, n_features, n_features)\n",
    "        Covariance.\n",
    "\n",
    "    p: np.array, (n_clusters, )\n",
    "        Cluster weight.\n",
    "\n",
    "    assignment: np.array, (n_spikes, )\n",
    "        Cluster assignments / labels for each spike\n",
    "\n",
    "    nbins: int\n",
    "        Number of bins in a lda histogram.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    proj: np.array, (n_bins, n_clusters, n_clusters)\n",
    "        computed lda histo# Comparing the cells in particular\n",
    "\n",
    "    bins: np.array, (n_bins)\n",
    "        bin times relative to center    #bins x 1\n",
    "    \"\"\"\n",
    "\n",
    "    # insert your code here\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # compute the optimal separating axes for each pair of clusters (2 pts)\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # normalise according to first cluster (1 pt)\n",
    "    # -------------------------------------------\n",
    "\n",
    "    # --------------------------------------\n",
    "    # plot histograms on optimal axis (1 pt)\n",
    "    # --------------------------------------\n",
    "\n",
    "    return proj, bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proj, bins = separation(b, m, S, p, a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
