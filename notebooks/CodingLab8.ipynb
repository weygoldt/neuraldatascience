{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Neural Data Science_\n",
    "\n",
    "Lecturer: Prof. Dr. Philipp Berens\n",
    "\n",
    "Tutors: Jonas Beck, Ziwei Huang, Rita González Márquez\n",
    "\n",
    "Summer term 2022\n",
    "\n",
    "Name: FILL IN YOUR NAMES HERE\n",
    "\n",
    "# Coding Lab 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "mpl.rc(\"savefig\", dpi=72)\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Implement entropy estimators\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General framework\n",
    "\n",
    "Entropy is defined as \n",
    "\n",
    "$$\n",
    "H[p] = -\\sum_x p_x \\log p_x\n",
    "$$\n",
    "\n",
    "where $p_x = p(x=X)$. Here we assume that $X$ is a discrete random variable and that there are finitely many states $K$ that $X$ can take.\n",
    "\n",
    "We are interested in the entropy of discrete random variables, because of its relationship with mutual information:\n",
    "\n",
    "$$\n",
    "I[X|Y] = H[X] - H[X|Y]\n",
    "$$\n",
    "\n",
    "If we can estimate the entropy well, we can estimate the mutual information well. An application in neuroscience would be estimating the mutual information between a spike train modeled as a sequence of $1$ s and $0$ s (e.g. $(0,1,0,1,1)$) and a discrete set of stimuli.\n",
    "\n",
    "Note that a multivariate binary distribution modeling a spike train can always be mapped to a discrete univariate distribution, $\\mathbb{Z}_2 \\longrightarrow \\mathbb{Z_+}$, by interpreting each binary state $z \\in \\mathbb{Z}_2$ as its corresponding binary number and computing $f(z) = \\sum_i 2^{i} z_i$.\n",
    "\n",
    "As discussed in the lecture, the problem is that one always underestimates the true entropy of a distribution from samples. In this exercise you are meant to implement different estimators for discrete entropy and evaluate them on different discrete distributions:\n",
    "\n",
    "* Uniform distribution: $p(x=X) = \\frac{1}{K}$\n",
    "\n",
    "* \"Zipf's law\"- distribution: $p(x=X) = \\frac{1}{Z x} $, where $Z = \\sum_k 1/k$\n",
    "\n",
    "There is a really good series of blog posts about discrete entropy estimation to be found [here](http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-1.html), [here](http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-2.html) and [here](http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-3.html). \n",
    "\n",
    "Make sure you use binary logarithms throughout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the estimators\n",
    "\n",
    "Implement the\n",
    "\n",
    "* maximum likelihood estimator (1 pt)\n",
    "* miller-maddow corrected estimator (1 pt)\n",
    "* jack-knife corrected estimator (2 pt)\n",
    "* coverage adjusted estimator (1 pt).\n",
    "\n",
    "When implementing the jack-knife estimator, you may want to restrict the amount of resampling for performance reasons e.g. to 1000, even if more samples are available. By definition, $0\\log0=0$. Adapt the interfaces as needed for your implementation.\n",
    "\n",
    "In addition, implement or use one of the following more advanced estimators (1+3 pts, extra points if you use your own implementation):\n",
    "\n",
    "* [JVHW estimator](https://arxiv.org/abs/1406.6956) with code on [github](https://github.com/EEthinker/JVHW_Entropy_Estimators/tree/master/Python)\n",
    "* [Unseen estimator](http://papers.nips.cc/paper/5170-estimating-the-unseen-improved-estimators-for-entropy-and-other-properties) (includes Matlab code in Supplementary)\n",
    "* [Best Upper Bounds estimator](http://www.mitpressjournals.org/doi/abs/10.1162/089976603321780272) (Matlab code available on Ilias)\n",
    "\n",
    "For this part, you are allowed to use an existing implementation as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE\n",
    "\n",
    "$H_{ML}= -\\sum_{x}\\hat{p}(x)log(\\hat{p}(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_mle(phat):\n",
    "    '''Maximum likelihood or plug-in estimator of discrete entropy\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    \n",
    "    phat: np.array, shape=(n_bins, )\n",
    "        Estimate of the distribution / histogram\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    H: float\n",
    "        Entropy estimate\n",
    "    '''\n",
    "    # insert your code here (1 pt)\n",
    "\n",
    "    \n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miller-Maddow corrected\n",
    "\n",
    "$H_{MM}=H_{ML}+\\frac{\\hat{d}-1}{2n}$ \n",
    "\n",
    "$ \\hat{d} = \\#[\\hat{p}(x)>0]$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_mm(phat, n):\n",
    "    '''Miller-Maddow corrected estimator of discrete entropy\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    phat: np.array, shape=(n_bins, )\n",
    "        Estimate of the distribution / histogram\n",
    "        \n",
    "    n:  int\n",
    "        Number of samples\n",
    "    \n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    \n",
    "    H: float\n",
    "        Entropy estimate\n",
    "    '''\n",
    "\n",
    "    # insert your code here (1 pt)\n",
    "\n",
    "    \n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jack-knife corrected\n",
    "\n",
    "$\\hat{H}_{JK} = N\\hat{H}_{ML} - (N-1)\\hat{H}_{ML}^{(.)}$ \n",
    "\n",
    "$\\hat{H}_{ML}^{(.)} = <H_{ML}^{\\lnot i}>$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_jk(x, edges):\n",
    "    '''Jack-knife corrected estimator of discrete entropy\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: np.array, shape=(n_samples, )\n",
    "        Samples\n",
    "        \n",
    "    edges: np.array, shape=(n_bins, )\n",
    "        Histogram bin edges\n",
    "    \n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    \n",
    "    H: float\n",
    "        Entropy estimate\n",
    "    '''\n",
    "\n",
    "    # insert your code here (2 pt)\n",
    "    \n",
    "    \n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage-adjusted\n",
    "\n",
    "$C = 1 - \\frac{\\# f_{i}=1}{N}$\n",
    "\n",
    "$\\hat{P}_{C}= \\hat{P}\\cdot C$ \n",
    "\n",
    "$H_{CA}= -\\sum_{x}\\frac{\\hat{P_{C}}(x)log(\\hat{P_{C}}(x))}{1-(1-\\hat{P_{C}}(x))^N}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_cae(phat, n):\n",
    "    '''coverage-adjusted estimator of discrete entropy\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    phat: np.array, shape=(n_bins, )\n",
    "        Estimate of the distribution / histogram\n",
    "        \n",
    "    n: int\n",
    "        Number of samples.\n",
    "    \n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    \n",
    "    H: float\n",
    "        Entropy estimate\n",
    "    '''\n",
    "\n",
    "    # insert your code here (1 pt)\n",
    "    \n",
    "    \n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JVHW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import est_entro as ee\n",
    "\n",
    "def entropy_jvhw(x):\n",
    "    '''JVHW estimator of discrete entropy.\n",
    "        \n",
    "    Parameter\n",
    "    ---------\n",
    "    x: np.array, shape=(n_samples, )\n",
    "        Samples\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    H: float\n",
    "        Entropy estimate\n",
    "    '''\n",
    "\n",
    "    # insert your code here (1 pt)\n",
    "\n",
    "    \n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Unseen or Best Upper Bounds estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here \n",
    "\n",
    "# ------------------------------------------\n",
    "# Port Unseen or Best Upper Bounds estimator\n",
    "# from MatLab to Python. (3 bonus pts)\n",
    "# ------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 10\n",
    "N = 2**D\n",
    "\n",
    "p = 1/N * np.ones(N)   # true distribution\n",
    "\n",
    "H = - np.sum(p * np.log2(p))  # true entropy\n",
    "\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from the uniform distribution using sample sizes of 100 and 10000. Plot the true distribution and the sampled distributions. What do you notice? (2 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here \n",
    "\n",
    "# ------------------------------------\n",
    "# Sample from the uniform distribution\n",
    "# using sample size of 100 (0.5 pts)\n",
    "# ------------------------------------\n",
    "\n",
    "# ------------------------------------\n",
    "# Sample from the uniform distribution\n",
    "# using sample size of 10000 (0.5 pts)\n",
    "# ------------------------------------\n",
    "\n",
    "# ------------------------------------\n",
    "# Plot the true distribution and \n",
    "# the sampled distributions. (0.5 pts)\n",
    "# ------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the framework below to generate samples of different size (logarithmically spaced between 10 and 100000) and evaluate the different entropy estimators for multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = np.round(np.logspace(1,5,num=10))\n",
    "n_runs = 30\n",
    "\n",
    "edges = np.arange(-0.5, N, 1)\n",
    "\n",
    "h_mle  = np.zeros((len(sample_sizes), n_runs))\n",
    "h_mm   = np.zeros((len(sample_sizes), n_runs))\n",
    "h_jk   = np.zeros((len(sample_sizes), n_runs))\n",
    "h_cae  = np.zeros((len(sample_sizes), n_runs))\n",
    "h_jvhw = np.zeros((len(sample_sizes), n_runs))\n",
    "\n",
    "# add h_unseen or h_bub here if you implemented them.\n",
    "\n",
    "for i, S in enumerate(sample_sizes):\n",
    "    for j in np.arange(n_runs):\n",
    "        \n",
    "        # ------------------------------------\n",
    "        # Sample from the uniform distribution\n",
    "        # with different sample size (0.5 pts)\n",
    "        # ------------------------------------  \n",
    "         \n",
    "        # insert your code here  \n",
    "        \n",
    "        h_mle[i,j]  = entropy_mle(phat)\n",
    "        h_mm[i,j]   = entropy_mm(phat, S)\n",
    "        h_cae[i,j]  = entropy_cae(phat, S)\n",
    "        h_jk[i,j]   = entropy_jk(x, edges)\n",
    "        h_jvhw[i,j] = entropy_jvhw(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the resulting average estimate of the entropy for each of the estimators. Which is best? If you implemented everything correctly, this plot should roughly look like in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.semilogx(sample_sizes, np.mean(h_mle,  axis=1), label='mle')\n",
    "plt.semilogx(sample_sizes, np.mean(h_mm,   axis=1), label='mm')\n",
    "plt.semilogx(sample_sizes, np.mean(h_cae,  axis=1), label='cae')\n",
    "plt.semilogx(sample_sizes, np.mean(h_jk,   axis=1), label='jk')\n",
    "plt.semilogx(sample_sizes, np.mean(h_jvhw, axis=1), label='jvhw')\n",
    "\n",
    "# plot h_unseen or h_bub here if you implemented them.\n",
    "\n",
    "plt.axhline(H, color='black', linestyle=':')\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipf distribution\n",
    "\n",
    "[Zipf's law ](https://en.wikipedia.org/wiki/Zipf%27s_law) refers to a family of power law like distributions for which $p_k \\sim 1/k^d$. We will simply use $d=1$ here.   \n",
    "\n",
    "Adapt the framework above to sample from a Zipf distribution and evaluate the estimators for this case. Are there differences to the uniform case? (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 10\n",
    "N = 2**D\n",
    "\n",
    "p = 1/(np.arange(0,N)+1)    # true distribution\n",
    "p = p/np.sum(p)\n",
    "\n",
    "H = - np.sum(p * np.log2(p))  # true entropy\n",
    "\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from the Zipf distribution using sample sizes of 100 and 10000. In this case, the function `random.choice` is very helpful for sampling. Plot the true distribution and the sampled distributions. What do you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here \n",
    "\n",
    "# ---------------------------------------\n",
    "# Sample from the Zipf distribution\n",
    "# using sample size of 100 (0.5 pts)\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Sample from the Zipf distribution\n",
    "# using sample size of 10000 (0.5 pts)\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Plot the true distribution and the sampled\n",
    "# distributions. (0.5 pts)\n",
    "# ---------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the framework below to generate samples of different size (logarithmically spaced between 10 and 100000) and evaluate the different entropy estimators for multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = np.round(np.logspace(1,5,num=10))\n",
    "n_runs = 30\n",
    "\n",
    "edges = np.arange(-0.5, N, 1)\n",
    "\n",
    "h_mle  = np.zeros((len(sample_sizes), n_runs))\n",
    "h_mm   = np.zeros((len(sample_sizes), n_runs))\n",
    "h_jk   = np.zeros((len(sample_sizes), n_runs))\n",
    "h_cae  = np.zeros((len(sample_sizes), n_runs))\n",
    "h_jvhw = np.zeros((len(sample_sizes), n_runs))\n",
    "\n",
    "# add h_unseen or h_bub here if you implemented them.\n",
    "\n",
    "for i, S in enumerate(sample_sizes):\n",
    "    for j in np.arange(n_runs):\n",
    "    \n",
    "        # ---------------------------------------\n",
    "        # Sample from the Zipf distribution\n",
    "        # with different sample size (0.5 pts)\n",
    "        # ---------------------------------------\n",
    "\n",
    "        # insert your code here   \n",
    "        \n",
    "        h_mle[i,j]  = entropy_mle(phat)\n",
    "        h_mm[i,j]   = entropy_mm(phat, S)\n",
    "        h_cae[i,j]  = entropy_cae(phat, S)\n",
    "        h_jk[i,j]   = entropy_jk(x, edges)\n",
    "        h_jvhw[i,j] = entropy_jvhw(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot resulting average estimate of the entropy for each of the estimators. Which is best? If you implemented everything correctly, this plot should roughly look like in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.semilogx(sample_sizes, np.mean(h_mle,  axis=1), label='mle')\n",
    "plt.semilogx(sample_sizes, np.mean(h_mm,   axis=1), label='mm')\n",
    "plt.semilogx(sample_sizes, np.mean(h_cae,  axis=1), label='cae')\n",
    "plt.semilogx(sample_sizes, np.mean(h_jk,   axis=1), label='jk')\n",
    "plt.semilogx(sample_sizes, np.mean(h_jvhw, axis=1), label='jvhw')\n",
    "\n",
    "# plot h_unseen or h_bub here if you implemented them.\n",
    "\n",
    "plt.axhline(H, color='black', linestyle=':')\n",
    "plt.legend()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "4a52f57b56c27f38fc3d95daa57af6da3929fe8541a384ec0d11efb6a1b206eb"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "nteract": {
   "version": "0.13.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
